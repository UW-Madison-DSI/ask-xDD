{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get a somewhat parsed full-text from xDD team, preferably had section label, citation label, etc.\n",
    "2. Query-based text summarization (Similar to Petal's AI table)\n",
    "3. Sort into support and counter arguments based on semantics\n",
    "4. Return sorted evidences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.044329166412353516,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading builder script",
       "rate": null,
       "total": null,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8a179ea3a24275bb94fc419f7d30c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03597259521484375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading metadata",
       "rate": null,
       "total": null,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a070e6b5d344094baeb7fb71fc2619c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.039029598236083984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading readme",
       "rate": null,
       "total": null,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa29101b38a452485d268ae25bc2b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset covid_qa_deepset/covid_qa_deepset to /root/.cache/huggingface/datasets/covid_qa_deepset/covid_qa_deepset/1.0.0/fb886523842e312176f92ec8e01e77a08fa15a694f5741af6fc42796ee9c8c46...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.031996726989746094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 1346251,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55f40ce1d7e473a9db23e06f2a8a6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.031981468200683594,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 2019,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f821aedf80a2485faf649b70a869ba06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2019 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset covid_qa_deepset downloaded and prepared to /root/.cache/huggingface/datasets/covid_qa_deepset/covid_qa_deepset/1.0.0/fb886523842e312176f92ec8e01e77a08fa15a694f5741af6fc42796ee9c8c46. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.037830352783203125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a580b0987dbd4830bc97f6ab90b53a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"covid_qa_deepset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document_id', 'context', 'question', 'is_impossible', 'id', 'answers'],\n",
       "        num_rows: 2019\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigscience/bloomz-7b1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = tokenizer.encode(\"Translate to English: Je tâ€™aime.\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install triton"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.autonotebook import tqdm\n",
    "# import pathlib\n",
    "# from pdf_parser import parse_pdf\n",
    "# pdfs = pathlib.Path('data/').glob('*.pdf')\n",
    "\n",
    "# for i, pdf in tqdm(enumerate(pdfs)):\n",
    "#     paper = parse_pdf(pdf)\n",
    "#     paper.save(f'data/parsed/{i}.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_parser import Paper\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load parsed PDF\n",
    "p = Paper.load(\"data/parsed/0.pkl.gz\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31061"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long T-5\n",
    "- https://huggingface.co/google/long-t5-tglobal-base\n",
    "- https://huggingface.co/google/long-t5-tglobal-xl\n",
    "- https://huggingface.co/google/long-t5-tglobal-large\n",
    "\n",
    "\n",
    "Relevant but not directly useful... also buggy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LongT5Model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-tglobal-base\")\n",
    "model = LongT5Model.from_pretrained(\"google/long-t5-tglobal-base\")\n",
    "\n",
    "inputs = tokenizer(\"Based on this context\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GPT summarization with `compress`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend import compress, get_answer\n",
    "\n",
    "# s = compress(p.chunks)\n",
    "# short_text = '\\n'.join(s)\n",
    "\n",
    "# with open('tmp/tmp.txt', 'w') as f:\n",
    "#     f.write(short_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT3.5 baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write short_text to short_text.txt\n",
    "\n",
    "with open(\"tmp/tmp.txt\", \"r\") as f:\n",
    "    short_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_answer(context=short_text, question=\"What is QSPR modeling workflow?\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset to small test toy example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-T5\n",
    "https://huggingface.co/google/long-t5-tglobal-base"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mt0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " | Params | Checkpoint |\n",
    " | ------ | ---------- |\n",
    " | 300M   | mt0-small  |\n",
    " | 580M   | mt0-base   |\n",
    " | 1.2B   | mt0-large  |\n",
    " | 3.7B   | mt0-xl     |\n",
    " | 13B    | mt0-xxl    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BloomZ\n",
    "\n",
    " | Params | Checkpoint  |\n",
    " | ------ | ----------- |\n",
    " | 560M   | bloomz-560m |\n",
    " | 1.1B   | bloomz-1b1  |\n",
    " | 1.7B   | bloomz-1b7  |\n",
    " | 3B     | bloomz-3b   |\n",
    " | 7.1B   | bloomz-7b1  |\n",
    " | 176B   | bloomz      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# checkpoint = \"bigscience/bloomz\"\n",
    "checkpoint = \"bigscience/bloomz-560m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\n",
    "    \"based on this article, what is QSPR modeling workflow? \" + short_text,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "outputs = model.generate(inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### m-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigscience/mt0-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    checkpoint, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\n",
    "    \"based on this article, what is QSPR modeling workflow? \" + short_text,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "outputs = model.generate(inputs, max_new_tokens=1000)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"dennlinger/bert-wiki-paragraphs\")\n",
    "pipe(\"{First paragraph} [SEP] {Second paragraph}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding from distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def get_sentence_embeddings(sentences, tokenizer, model):\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
