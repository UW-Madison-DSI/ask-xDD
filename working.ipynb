{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning prototype\n",
    "\n",
    "Goal: Fine-tune Long-t5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    LongT5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from askem.data import COVID_QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/long-t5-tglobal-base\"\n",
    "\n",
    "dataset = COVID_QA.train_test_split(test_size=0.2, seed=2023)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples: Dataset) -> dict:\n",
    "    \"\"\"Preprocess dataset.\n",
    "    Args:\n",
    "        example: A dict with keys ['context', 'question', 'answers'].\n",
    "\n",
    "    Returns:\n",
    "        model_input: with dict_keys(['input_ids', 'attention_mask', 'labels']).\n",
    "    \"\"\"\n",
    "\n",
    "    questions = examples[\"question\"]\n",
    "    contexts = examples[\"context\"]\n",
    "    answers = [ans[\"text\"][0] for ans in examples[\"answers\"]]\n",
    "\n",
    "    # Inputs\n",
    "    input_texts = [f\"question: {q} context: {c}\" for q, c in zip(questions, contexts)]\n",
    "    model_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        max_length=16384,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    # Labels\n",
    "    labels = tokenizer(\n",
    "        answers, max_length=1000, truncation=True, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.map(preprocess, batched=True)\n",
    "test_data = test_data.select_columns([\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_data = test_data.with_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from askem.data import TestResults\n",
    "\n",
    "\n",
    "def benchmark(model, dataset, orm, model_id: str):\n",
    "    \"\"\"Benchmark a model on a dataset and push to ORM.\n",
    "\n",
    "    The ORM should have a table with the following text columns:\n",
    "        - model_id\n",
    "        - context\n",
    "        - question\n",
    "        - true_answer\n",
    "        - pred_answer\n",
    "    \"\"\"\n",
    "\n",
    "    y = model.generate(dataset[\"input_ids\"], attention_mask=dataset[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(test_data[\"input_ids\"], attention_mask=test_data[\"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_data[\"input_ids\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(\n",
    "    test_data[\"input_ids\"][0].unsqueeze(0),\n",
    "    attention_mask=test_data[\"attention_mask\"][0].unsqueeze(0),\n",
    "    max_length=512,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=test_data,  # DEBUG MODE\n",
    "    eval_dataset=test_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[run_seq2seq_qa](https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_seq2seq_qa.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from trainer_seq2seq_qa import QuestionAnsweringSeq2SeqTrainer\n",
    "import math\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers.trainer_utils import PredictionOutput, speed_metrics\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import (\n",
    "    EvalLoopOutput,\n",
    "    EvalPrediction,\n",
    "    get_last_checkpoint,\n",
    ")\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnsweringSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def __init__(self, *args, eval_examples=None, post_process_function=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.eval_examples = eval_examples\n",
    "        self.post_process_function = post_process_function\n",
    "\n",
    "    # def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metric_key_prefix: str = \"eval\"):\n",
    "    def evaluate(\n",
    "        self,\n",
    "        eval_dataset: Optional[Dataset] = None,\n",
    "        eval_examples=None,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "        metric_key_prefix: str = \"eval\",\n",
    "        **gen_kwargs,\n",
    "    ) -> Dict[str, float]:\n",
    "        gen_kwargs = gen_kwargs.copy()\n",
    "        gen_kwargs[\"max_length\"] = (\n",
    "            gen_kwargs[\"max_length\"]\n",
    "            if gen_kwargs.get(\"max_length\") is not None\n",
    "            else self.args.generation_max_length\n",
    "        )\n",
    "        gen_kwargs[\"num_beams\"] = (\n",
    "            gen_kwargs[\"num_beams\"]\n",
    "            if gen_kwargs.get(\"num_beams\") is not None\n",
    "            else self.args.generation_num_beams\n",
    "        )\n",
    "        self._gen_kwargs = gen_kwargs\n",
    "\n",
    "        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset\n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "        eval_examples = self.eval_examples if eval_examples is None else eval_examples\n",
    "\n",
    "        # Temporarily disable metric computation, we will do it in the loop here.\n",
    "        compute_metrics = self.compute_metrics\n",
    "        self.compute_metrics = None\n",
    "        start_time = time.time()\n",
    "        eval_loop = (\n",
    "            self.prediction_loop\n",
    "            if self.args.use_legacy_prediction_loop\n",
    "            else self.evaluation_loop\n",
    "        )\n",
    "        try:\n",
    "            output = eval_loop(\n",
    "                eval_dataloader,\n",
    "                description=\"Evaluation\",\n",
    "                # No point gathering the predictions if there are no metrics, otherwise we defer to\n",
    "                # self.args.prediction_loss_only\n",
    "                prediction_loss_only=True if compute_metrics is None else None,\n",
    "                ignore_keys=ignore_keys,\n",
    "                metric_key_prefix=metric_key_prefix,\n",
    "            )\n",
    "        finally:\n",
    "            self.compute_metrics = compute_metrics\n",
    "        total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
    "        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n",
    "            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n",
    "        output.metrics.update(\n",
    "            speed_metrics(\n",
    "                metric_key_prefix,\n",
    "                start_time,\n",
    "                num_samples=output.num_samples,\n",
    "                num_steps=math.ceil(output.num_samples / total_batch_size),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            self.post_process_function is not None\n",
    "            and self.compute_metrics is not None\n",
    "            and self.args.should_save\n",
    "        ):\n",
    "            # Only the main node write the results by default\n",
    "            eval_preds = self.post_process_function(eval_examples, eval_dataset, output)\n",
    "            metrics = self.compute_metrics(eval_preds)\n",
    "\n",
    "            # Prefix all keys with metric_key_prefix + '_'\n",
    "            for key in list(metrics.keys()):\n",
    "                if not key.startswith(f\"{metric_key_prefix}_\"):\n",
    "                    metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
    "\n",
    "            metrics.update(output.metrics)\n",
    "        else:\n",
    "            metrics = output.metrics\n",
    "\n",
    "        if self.args.should_log:\n",
    "            # Only the main node log the results by default\n",
    "            self.log(metrics)\n",
    "\n",
    "        if self.args.tpu_metrics_debug or self.args.debug:\n",
    "            # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "            xm.master_print(met.metrics_report())\n",
    "\n",
    "        self.control = self.callback_handler.on_evaluate(\n",
    "            self.args, self.state, self.control, metrics\n",
    "        )\n",
    "        return metrics\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        predict_dataset,\n",
    "        predict_examples,\n",
    "        ignore_keys=None,\n",
    "        metric_key_prefix: str = \"test\",\n",
    "        **gen_kwargs,\n",
    "    ):\n",
    "        self._gen_kwargs = gen_kwargs.copy()\n",
    "\n",
    "        predict_dataloader = self.get_test_dataloader(predict_dataset)\n",
    "\n",
    "        # Temporarily disable metric computation, we will do it in the loop here.\n",
    "        compute_metrics = self.compute_metrics\n",
    "        self.compute_metrics = None\n",
    "        start_time = time.time()\n",
    "        eval_loop = (\n",
    "            self.prediction_loop\n",
    "            if self.args.use_legacy_prediction_loop\n",
    "            else self.evaluation_loop\n",
    "        )\n",
    "        try:\n",
    "            output = eval_loop(\n",
    "                predict_dataloader,\n",
    "                description=\"Prediction\",\n",
    "                # No point gathering the predictions if there are no metrics, otherwise we defer to\n",
    "                # self.args.prediction_loss_only\n",
    "                prediction_loss_only=True if compute_metrics is None else None,\n",
    "                ignore_keys=ignore_keys,\n",
    "                metric_key_prefix=metric_key_prefix,\n",
    "            )\n",
    "        finally:\n",
    "            self.compute_metrics = compute_metrics\n",
    "\n",
    "        total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
    "        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n",
    "            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n",
    "        output.metrics.update(\n",
    "            speed_metrics(\n",
    "                metric_key_prefix,\n",
    "                start_time,\n",
    "                num_samples=output.num_samples,\n",
    "                num_steps=math.ceil(output.num_samples / total_batch_size),\n",
    "            )\n",
    "        )\n",
    "        if self.post_process_function is None or self.compute_metrics is None:\n",
    "            return output\n",
    "\n",
    "        predictions = self.post_process_function(\n",
    "            predict_examples, predict_dataset, output, \"predict\"\n",
    "        )\n",
    "        metrics = self.compute_metrics(predictions)\n",
    "\n",
    "        # Prefix all keys with metric_key_prefix + '_'\n",
    "        for key in list(metrics.keys()):\n",
    "            if not key.startswith(f\"{metric_key_prefix}_\"):\n",
    "                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
    "        metrics.update(output.metrics)\n",
    "        return PredictionOutput(\n",
    "            predictions=predictions.predictions,\n",
    "            label_ids=predictions.label_ids,\n",
    "            metrics=metrics,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.30.0.dev0\")\n",
    "\n",
    "require_version(\n",
    "    \"datasets>=1.8.0\",\n",
    "    \"To fix: pip install -r examples/pytorch/question-answering/requirements.txt\",\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\n",
    "            \"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"\n",
    "        }\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained config name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained tokenizer name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Path to directory to store the pretrained models downloaded from huggingface.co\"\n",
    "        },\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"\n",
    "        },\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\n",
    "            \"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"\n",
    "        },\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"},\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The configuration name of the dataset to use (via the datasets library).\"\n",
    "        },\n",
    "    )\n",
    "    context_column: Optional[str] = field(\n",
    "        default=\"context\",\n",
    "        metadata={\n",
    "            \"help\": \"The name of the column in the datasets containing the contexts (for question answering).\"\n",
    "        },\n",
    "    )\n",
    "    question_column: Optional[str] = field(\n",
    "        default=\"question\",\n",
    "        metadata={\n",
    "            \"help\": \"The name of the column in the datasets containing the questions (for question answering).\"\n",
    "        },\n",
    "    )\n",
    "    answer_column: Optional[str] = field(\n",
    "        default=\"answers\",\n",
    "        metadata={\n",
    "            \"help\": \"The name of the column in the datasets containing the answers (for question answering).\"\n",
    "        },\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a text file).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"\n",
    "        },\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"An optional input test data file to evaluate the perplexity on (a text file).\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Overwrite the cached training and evaluation sets\"},\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=384,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_answer_length: int = field(\n",
    "        default=30,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum length of an answer that can be generated. This is needed because the start \"\n",
    "                \"and end predictions are not conditioned on one another.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    val_max_answer_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded. Will default to `max_answer_length`.\"\n",
    "                \"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used \"\n",
    "                \"during ``evaluate`` and ``predict``.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to pad all samples to `max_seq_length`. If False, will pad the samples dynamically when\"\n",
    "                \" batching to the maximum length in the batch (which can be faster on GPU but will be slower on TPU).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    version_2_with_negative: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"If true, some of the examples do not have an answer.\"},\n",
    "    )\n",
    "    null_score_diff_threshold: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The threshold used to select the null answer: if the best answer has a score that is less than \"\n",
    "                \"the score of the null answer minus this threshold, the null answer is selected for this example. \"\n",
    "                \"Only useful when `version_2_with_negative=True`.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    doc_stride: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"When splitting up a long document into chunks, how much stride to take between chunks.\"\n",
    "        },\n",
    "    )\n",
    "    n_best_size: int = field(\n",
    "        default=20,\n",
    "        metadata={\n",
    "            \"help\": \"The total number of n-best predictions to generate when looking for an answer.\"\n",
    "        },\n",
    "    )\n",
    "    num_beams: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, \"\n",
    "                \"which is used during ``evaluate`` and ``predict``.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    ignore_pad_token_for_loss: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if (\n",
    "            self.dataset_name is None\n",
    "            and self.train_file is None\n",
    "            and self.validation_file is None\n",
    "            and self.test_file is None\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Need either a dataset name or a training/validation file/test_file.\"\n",
    "            )\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\n",
    "                    \"csv\",\n",
    "                    \"json\",\n",
    "                ], \"`train_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\n",
    "                    \"csv\",\n",
    "                    \"json\",\n",
    "                ], \"`validation_file` should be a csv or a json file.\"\n",
    "            if self.test_file is not None:\n",
    "                extension = self.test_file.split(\".\")[-1]\n",
    "                assert extension in [\n",
    "                    \"csv\",\n",
    "                    \"json\",\n",
    "                ], \"`test_file` should be a csv or a json file.\"\n",
    "        if self.val_max_answer_length is None:\n",
    "            self.val_max_answer_length = self.max_answer_length\n",
    "\n",
    "\n",
    "question_answering_column_name_mapping = {\n",
    "    \"squad_v2\": (\"question\", \"context\", \"answer\"),\n",
    "}\n",
    "\n",
    "\n",
    "def main():\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "    parser = HfArgumentParser(\n",
    "        (ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments)\n",
    "    )\n",
    "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "        # If we pass only one argument to the script and it's the path to a json file,\n",
    "        # let's parse it to get our arguments.\n",
    "        model_args, data_args, training_args = parser.parse_json_file(\n",
    "            json_file=os.path.abspath(sys.argv[1])\n",
    "        )\n",
    "    else:\n",
    "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "\n",
    "    if training_args.should_log:\n",
    "        # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "    log_level = training_args.get_process_log_level()\n",
    "    logger.setLevel(log_level)\n",
    "    datasets.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "    # Log on each process the small summary:\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "    # Detecting last checkpoint.\n",
    "    last_checkpoint = None\n",
    "    if (\n",
    "        os.path.isdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif (\n",
    "            last_checkpoint is not None and training_args.resume_from_checkpoint is None\n",
    "        ):\n",
    "            logger.info(\n",
    "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "\n",
    "    # Set seed before initializing model.\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
    "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
    "    #\n",
    "    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n",
    "    # 'text' is found. You can easily tweak this behavior (see below).\n",
    "    #\n",
    "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "    # download the dataset.\n",
    "    if data_args.dataset_name is not None:\n",
    "        # Downloading and loading a dataset from the hub.\n",
    "        raw_datasets = load_dataset(\n",
    "            data_args.dataset_name,\n",
    "            data_args.dataset_config_name,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "        )\n",
    "    else:\n",
    "        data_files = {}\n",
    "        if data_args.train_file is not None:\n",
    "            data_files[\"train\"] = data_args.train_file\n",
    "            extension = data_args.train_file.split(\".\")[-1]\n",
    "        if data_args.validation_file is not None:\n",
    "            data_files[\"validation\"] = data_args.validation_file\n",
    "            extension = data_args.validation_file.split(\".\")[-1]\n",
    "        if data_args.test_file is not None:\n",
    "            data_files[\"test\"] = data_args.test_file\n",
    "            extension = data_args.test_file.split(\".\")[-1]\n",
    "        raw_datasets = load_dataset(\n",
    "            extension,\n",
    "            data_files=data_files,\n",
    "            field=\"data\",\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "        )\n",
    "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # Distributed training:\n",
    "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name\n",
    "        if model_args.config_name\n",
    "        else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name\n",
    "        if model_args.tokenizer_name\n",
    "        else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_fast=model_args.use_fast_tokenizer,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "\n",
    "    # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "    # on a small vocab and want a smaller embedding size, remove this test.\n",
    "    embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "    if len(tokenizer) > embedding_size:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if model.config.decoder_start_token_id is None:\n",
    "        raise ValueError(\n",
    "            \"Make sure that `config.decoder_start_token_id` is correctly defined\"\n",
    "        )\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to generate and tokenize inputs and targets.\n",
    "    if training_args.do_train:\n",
    "        column_names = raw_datasets[\"train\"].column_names\n",
    "    elif training_args.do_eval:\n",
    "        column_names = raw_datasets[\"validation\"].column_names\n",
    "    elif training_args.do_predict:\n",
    "        column_names = raw_datasets[\"test\"].column_names\n",
    "    else:\n",
    "        logger.info(\n",
    "            \"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # Get the column names for input/target.\n",
    "    dataset_columns = question_answering_column_name_mapping.get(\n",
    "        data_args.dataset_name, None\n",
    "    )\n",
    "    if data_args.question_column is None:\n",
    "        question_column = (\n",
    "            dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
    "        )\n",
    "    else:\n",
    "        question_column = data_args.question_column\n",
    "        if question_column not in column_names:\n",
    "            raise ValueError(\n",
    "                f\"--question_column' value '{data_args.question_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "            )\n",
    "    if data_args.context_column is None:\n",
    "        context_column = (\n",
    "            dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
    "        )\n",
    "    else:\n",
    "        context_column = data_args.context_column\n",
    "        if context_column not in column_names:\n",
    "            raise ValueError(\n",
    "                f\"--context_column' value '{data_args.context_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "            )\n",
    "    if data_args.answer_column is None:\n",
    "        answer_column = (\n",
    "            dataset_columns[2] if dataset_columns is not None else column_names[2]\n",
    "        )\n",
    "    else:\n",
    "        answer_column = data_args.answer_column\n",
    "        if answer_column not in column_names:\n",
    "            raise ValueError(\n",
    "                f\"--answer_column' value '{data_args.answer_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "            )\n",
    "\n",
    "    # Temporarily set max_answer_length for training.\n",
    "    max_answer_length = data_args.max_answer_length\n",
    "    padding = \"max_length\" if data_args.pad_to_max_length else False\n",
    "\n",
    "    if training_args.label_smoothing_factor > 0 and not hasattr(\n",
    "        model, \"prepare_decoder_input_ids_from_labels\"\n",
    "    ):\n",
    "        logger.warning(\n",
    "            \"label_smoothing is enabled but the `prepare_decoder_input_ids_from_labels` method is not defined for\"\n",
    "            f\"`{model.__class__.__name__}`. This will lead to loss being calculated twice and will take up more memory\"\n",
    "        )\n",
    "\n",
    "    if data_args.max_seq_length > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n",
    "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "    def preprocess_squad_batch(\n",
    "        examples,\n",
    "        question_column: str,\n",
    "        context_column: str,\n",
    "        answer_column: str,\n",
    "    ) -> Tuple[List[str], List[str]]:\n",
    "        questions = examples[question_column]\n",
    "        contexts = examples[context_column]\n",
    "        answers = examples[answer_column]\n",
    "\n",
    "        def generate_input(_question, _context):\n",
    "            return \" \".join(\n",
    "                [\"question:\", _question.lstrip(), \"context:\", _context.lstrip()]\n",
    "            )\n",
    "\n",
    "        inputs = [\n",
    "            generate_input(question, context)\n",
    "            for question, context in zip(questions, contexts)\n",
    "        ]\n",
    "        targets = [\n",
    "            answer[\"text\"][0] if len(answer[\"text\"]) > 0 else \"\" for answer in answers\n",
    "        ]\n",
    "        return inputs, targets\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        inputs, targets = preprocess_squad_batch(\n",
    "            examples, question_column, context_column, answer_column\n",
    "        )\n",
    "\n",
    "        model_inputs = tokenizer(\n",
    "            inputs, max_length=max_seq_length, padding=padding, truncation=True\n",
    "        )\n",
    "        # Tokenize targets with text_target=...\n",
    "        labels = tokenizer(\n",
    "            text_target=targets,\n",
    "            max_length=max_answer_length,\n",
    "            padding=padding,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "        # padding in the loss.\n",
    "        if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
    "            labels[\"input_ids\"] = [\n",
    "                [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "                for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    # Validation preprocessing\n",
    "    def preprocess_validation_function(examples):\n",
    "        inputs, targets = preprocess_squad_batch(\n",
    "            examples, question_column, context_column, answer_column\n",
    "        )\n",
    "\n",
    "        model_inputs = tokenizer(\n",
    "            inputs,\n",
    "            max_length=max_seq_length,\n",
    "            padding=padding,\n",
    "            truncation=True,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        # Tokenize targets with the `text_target` keyword argument\n",
    "        labels = tokenizer(\n",
    "            text_target=targets,\n",
    "            max_length=max_answer_length,\n",
    "            padding=padding,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "        # padding in the loss.\n",
    "        if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
    "            labels[\"input_ids\"] = [\n",
    "                [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "                for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "\n",
    "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "        # its corresponding example. This key gives us just that.\n",
    "        sample_mapping = model_inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "        # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
    "        # corresponding example_id and we will store the offset mappings.\n",
    "        model_inputs[\"example_id\"] = []\n",
    "        # Augment the overflowing tokens to the labels\n",
    "        labels_out = []\n",
    "\n",
    "        for i in range(len(model_inputs[\"input_ids\"])):\n",
    "            # One example can give several spans, this is the index of the example containing this span of text.\n",
    "            sample_index = sample_mapping[i]\n",
    "            model_inputs[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "            labels_out.append(labels[\"input_ids\"][sample_index])\n",
    "\n",
    "        model_inputs[\"labels\"] = labels_out\n",
    "        return model_inputs\n",
    "\n",
    "    if training_args.do_train:\n",
    "        if \"train\" not in raw_datasets:\n",
    "            raise ValueError(\"--do_train requires a train dataset\")\n",
    "        train_dataset = raw_datasets[\"train\"]\n",
    "        if data_args.max_train_samples is not None:\n",
    "            # We will select sample from whole data if agument is specified\n",
    "            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "            train_dataset = train_dataset.select(range(max_train_samples))\n",
    "        # Create train feature from dataset\n",
    "        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
    "            train_dataset = train_dataset.map(\n",
    "                preprocess_function,\n",
    "                batched=True,\n",
    "                num_proc=data_args.preprocessing_num_workers,\n",
    "                remove_columns=column_names,\n",
    "                load_from_cache_file=not data_args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on train dataset\",\n",
    "            )\n",
    "        if data_args.max_train_samples is not None:\n",
    "            # Number of samples might increase during Feature Creation, We select only specified max samples\n",
    "            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "            train_dataset = train_dataset.select(range(max_train_samples))\n",
    "\n",
    "    if training_args.do_eval:\n",
    "        if \"validation\" not in raw_datasets:\n",
    "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "        eval_examples = raw_datasets[\"validation\"]\n",
    "        if data_args.max_eval_samples is not None:\n",
    "            # We will select sample from whole data\n",
    "            max_eval_samples = min(len(eval_examples), data_args.max_eval_samples)\n",
    "            eval_examples = eval_examples.select(range(max_eval_samples))\n",
    "        # Validation Feature Creation\n",
    "        with training_args.main_process_first(\n",
    "            desc=\"validation dataset map pre-processing\"\n",
    "        ):\n",
    "            eval_dataset = eval_examples.map(\n",
    "                preprocess_validation_function,\n",
    "                batched=True,\n",
    "                num_proc=data_args.preprocessing_num_workers,\n",
    "                remove_columns=column_names,\n",
    "                load_from_cache_file=not data_args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on validation dataset\",\n",
    "            )\n",
    "        if data_args.max_eval_samples is not None:\n",
    "            # During Feature creation dataset samples might increase, we will select required samples again\n",
    "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "\n",
    "    if training_args.do_predict:\n",
    "        if \"test\" not in raw_datasets:\n",
    "            raise ValueError(\"--do_predict requires a test dataset\")\n",
    "        predict_examples = raw_datasets[\"test\"]\n",
    "        if data_args.max_predict_samples is not None:\n",
    "            # We will select sample from whole data\n",
    "            predict_examples = predict_examples.select(\n",
    "                range(data_args.max_predict_samples)\n",
    "            )\n",
    "        # Predict Feature Creation\n",
    "        with training_args.main_process_first(\n",
    "            desc=\"prediction dataset map pre-processing\"\n",
    "        ):\n",
    "            predict_dataset = predict_examples.map(\n",
    "                preprocess_validation_function,\n",
    "                batched=True,\n",
    "                num_proc=data_args.preprocessing_num_workers,\n",
    "                remove_columns=column_names,\n",
    "                load_from_cache_file=not data_args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on prediction dataset\",\n",
    "            )\n",
    "        if data_args.max_predict_samples is not None:\n",
    "            # During Feature creation dataset samples might increase, we will select required samples again\n",
    "            max_predict_samples = min(\n",
    "                len(predict_dataset), data_args.max_predict_samples\n",
    "            )\n",
    "            predict_dataset = predict_dataset.select(range(max_predict_samples))\n",
    "\n",
    "    # Data collator\n",
    "    label_pad_token_id = (\n",
    "        -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
    "    )\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "    )\n",
    "\n",
    "    metric = evaluate.load(\"squad_v2\" if data_args.version_2_with_negative else \"squad\")\n",
    "\n",
    "    def compute_metrics(p: EvalPrediction):\n",
    "        return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
    "\n",
    "    # Post-processing:\n",
    "    def post_processing_function(\n",
    "        examples: datasets.Dataset,\n",
    "        features: datasets.Dataset,\n",
    "        outputs: EvalLoopOutput,\n",
    "        stage=\"eval\",\n",
    "    ):\n",
    "        # Decode the predicted tokens.\n",
    "        preds = outputs.predictions\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        # Replace -100s used for padding as we can't decode them\n",
    "        preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "        # Build a map example to its corresponding features.\n",
    "        example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "        feature_per_example = {\n",
    "            example_id_to_index[feature[\"example_id\"]]: i\n",
    "            for i, feature in enumerate(features)\n",
    "        }\n",
    "        predictions = {}\n",
    "        # Let's loop over all the examples!\n",
    "        for example_index, example in enumerate(examples):\n",
    "            # This is the index of the feature associated to the current example.\n",
    "            feature_index = feature_per_example[example_index]\n",
    "            predictions[example[\"id\"]] = decoded_preds[feature_index]\n",
    "\n",
    "        # Format the result to the format the metric expects.\n",
    "        if data_args.version_2_with_negative:\n",
    "            formatted_predictions = [\n",
    "                {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0}\n",
    "                for k, v in predictions.items()\n",
    "            ]\n",
    "        else:\n",
    "            formatted_predictions = [\n",
    "                {\"id\": k, \"prediction_text\": v} for k, v in predictions.items()\n",
    "            ]\n",
    "\n",
    "        references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column]} for ex in examples]\n",
    "        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    trainer = QuestionAnsweringSeq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset if training_args.do_train else None,\n",
    "        eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "        eval_examples=eval_examples if training_args.do_eval else None,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "        if training_args.predict_with_generate\n",
    "        else None,\n",
    "        post_process_function=post_processing_function,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        checkpoint = None\n",
    "        if training_args.resume_from_checkpoint is not None:\n",
    "            checkpoint = training_args.resume_from_checkpoint\n",
    "        elif last_checkpoint is not None:\n",
    "            checkpoint = last_checkpoint\n",
    "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "        trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "        metrics = train_result.metrics\n",
    "        max_train_samples = (\n",
    "            data_args.max_train_samples\n",
    "            if data_args.max_train_samples is not None\n",
    "            else len(train_dataset)\n",
    "        )\n",
    "        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    max_length = (\n",
    "        training_args.generation_max_length\n",
    "        if training_args.generation_max_length is not None\n",
    "        else data_args.val_max_answer_length\n",
    "    )\n",
    "    num_beams = (\n",
    "        data_args.num_beams\n",
    "        if data_args.num_beams is not None\n",
    "        else training_args.generation_num_beams\n",
    "    )\n",
    "    if training_args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "        metrics = trainer.evaluate(\n",
    "            max_length=max_length, num_beams=num_beams, metric_key_prefix=\"eval\"\n",
    "        )\n",
    "\n",
    "        max_eval_samples = (\n",
    "            data_args.max_eval_samples\n",
    "            if data_args.max_eval_samples is not None\n",
    "            else len(eval_dataset)\n",
    "        )\n",
    "        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "        trainer.log_metrics(\"eval\", metrics)\n",
    "        trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "    # Prediction\n",
    "    if training_args.do_predict:\n",
    "        logger.info(\"*** Predict ***\")\n",
    "        results = trainer.predict(predict_dataset, predict_examples)\n",
    "        metrics = results.metrics\n",
    "\n",
    "        max_predict_samples = (\n",
    "            data_args.max_predict_samples\n",
    "            if data_args.max_predict_samples is not None\n",
    "            else len(predict_dataset)\n",
    "        )\n",
    "        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n",
    "\n",
    "        trainer.log_metrics(\"predict\", metrics)\n",
    "        trainer.save_metrics(\"predict\", metrics)\n",
    "\n",
    "    if training_args.push_to_hub:\n",
    "        kwargs = {\n",
    "            \"finetuned_from\": model_args.model_name_or_path,\n",
    "            \"tasks\": \"question-answering\",\n",
    "        }\n",
    "        if data_args.dataset_name is not None:\n",
    "            kwargs[\"dataset_tags\"] = data_args.dataset_name\n",
    "            if data_args.dataset_config_name is not None:\n",
    "                kwargs[\"dataset_args\"] = data_args.dataset_config_name\n",
    "                kwargs[\n",
    "                    \"dataset\"\n",
    "                ] = f\"{data_args.dataset_name} {data_args.dataset_config_name}\"\n",
    "            else:\n",
    "                kwargs[\"dataset\"] = data_args.dataset_name\n",
    "\n",
    "        trainer.push_to_hub(**kwargs)\n",
    "\n",
    "\n",
    "def _mp_fn(index):\n",
    "    # For xla_spawn (TPUs)\n",
    "    main()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
