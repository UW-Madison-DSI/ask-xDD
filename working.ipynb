{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get a somewhat parsed full-text from xDD team, preferably had section label, citation label, etc.\n",
    "2. Query-based text summarization (Similar to Petal's AI table)\n",
    "3. Sort into support and counter arguments based on semantics\n",
    "4. Return sorted evidences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever, Seq2SeqGenerator\n",
    "from haystack.pipelines import GenerativeQAPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from askem.preprocessing import TextProcessor, convert_files_to_docs\n",
    "\n",
    "tp = TextProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = \"data/covid1000/\"\n",
    "docs = convert_files_to_docs(\n",
    "    dir_path=doc_dir, clean_func=tp.to_paragraphs, split_paragraphs=True\n",
    ")\n",
    "\n",
    "document_store = FAISSDocumentStore(embedding_dim=128, faiss_index_factory_str=\"Flat\")\n",
    "document_store.write_documents(docs)\n",
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"vblagoje/dpr-question_encoder-single-lfqa-wiki\",\n",
    "    passage_embedding_model=\"vblagoje/dpr-ctx_encoder-single-lfqa-wiki\",\n",
    ")\n",
    "document_store.update_embeddings(retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Seq2SeqGenerator(model_name_or_path=\"vblagoje/bart_lfqa\")\n",
    "pipe = GenerativeQAPipeline(generator, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pipe.run(\"Is covid chloroquine treatment effective?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[\"query\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[\"answers\"][0].__dict__.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y[\"answers\"][0].meta[\"content\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"covid_qa_deepset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigscience/bloomz-7b1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = tokenizer.encode(\"Translate to English: Je tâ€™aime.\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install triton"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.autonotebook import tqdm\n",
    "# import pathlib\n",
    "# from pdf_parser import parse_pdf\n",
    "# pdfs = pathlib.Path('data/').glob('*.pdf')\n",
    "\n",
    "# for i, pdf in tqdm(enumerate(pdfs)):\n",
    "#     paper = parse_pdf(pdf)\n",
    "#     paper.save(f'data/parsed/{i}.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_parser import Paper\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load parsed PDF\n",
    "p = Paper.load(\"data/parsed/0.pkl.gz\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long T-5\n",
    "- https://huggingface.co/google/long-t5-tglobal-base\n",
    "- https://huggingface.co/google/long-t5-tglobal-xl\n",
    "- https://huggingface.co/google/long-t5-tglobal-large\n",
    "\n",
    "\n",
    "Relevant but not directly useful... also buggy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LongT5Model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-tglobal-base\")\n",
    "model = LongT5Model.from_pretrained(\"google/long-t5-tglobal-base\")\n",
    "\n",
    "inputs = tokenizer(\"Based on this context\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GPT summarization with `compress`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend import compress, get_answer\n",
    "\n",
    "# s = compress(p.chunks)\n",
    "# short_text = '\\n'.join(s)\n",
    "\n",
    "# with open('tmp/tmp.txt', 'w') as f:\n",
    "#     f.write(short_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT3.5 baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write short_text to short_text.txt\n",
    "\n",
    "with open(\"tmp/tmp.txt\", \"r\") as f:\n",
    "    short_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_answer(context=short_text, question=\"What is QSPR modeling workflow?\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset to small test toy example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-T5\n",
    "https://huggingface.co/google/long-t5-tglobal-base"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mt0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " | Params | Checkpoint |\n",
    " | ------ | ---------- |\n",
    " | 300M   | mt0-small  |\n",
    " | 580M   | mt0-base   |\n",
    " | 1.2B   | mt0-large  |\n",
    " | 3.7B   | mt0-xl     |\n",
    " | 13B    | mt0-xxl    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BloomZ\n",
    "\n",
    " | Params | Checkpoint  |\n",
    " | ------ | ----------- |\n",
    " | 560M   | bloomz-560m |\n",
    " | 1.1B   | bloomz-1b1  |\n",
    " | 1.7B   | bloomz-1b7  |\n",
    " | 3B     | bloomz-3b   |\n",
    " | 7.1B   | bloomz-7b1  |\n",
    " | 176B   | bloomz      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# checkpoint = \"bigscience/bloomz\"\n",
    "checkpoint = \"bigscience/bloomz-560m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\n",
    "    \"based on this article, what is QSPR modeling workflow? \" + short_text,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "outputs = model.generate(inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### m-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigscience/mt0-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    checkpoint, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\n",
    "    \"based on this article, what is QSPR modeling workflow? \" + short_text,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "outputs = model.generate(inputs, max_new_tokens=1000)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"dennlinger/bert-wiki-paragraphs\")\n",
    "pipe(\"{First paragraph} [SEP] {Second paragraph}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding from distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def get_sentence_embeddings(sentences, tokenizer, model):\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph cleaner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert text into sentences\n",
    "2. Use `bert-wiki-paragraphs` to classify whether last sentence is talking about the same topic or not\n",
    "3. If same, concatenate\n",
    "4. If not, start a new paragraph\n",
    "5. If a paragraph is too long, start a new paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"dennlinger/bert-wiki-paragraphs\")\n",
    "\n",
    "pipe(\"{First paragraph} [SEP] {Second paragraph}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
