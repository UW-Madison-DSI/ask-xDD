{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AV-qqyz9ff8b"
   },
   "source": [
    "# Fine-tuning prototype\n",
    "\n",
    "Goal: Fine-tune Long-t5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongT5ForConditionalGeneration, AutoTokenizer\n",
    "from askem.data import get_covid_qa\n",
    "\n",
    "model_name = \"google/long-t5-tglobal-base\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dataset = get_covid_qa(split=\"test\")\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model = model.to(device).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(batch) -> dict:\n",
    "    \"\"\"A function to generate answer from batch dataset.\n",
    "    Args:\n",
    "        batch: A hugging face dataset.\n",
    "\n",
    "    Returns:\n",
    "        batch: A hugging face dataset with predicted answers.\n",
    "    \"\"\"\n",
    "\n",
    "    def _to_input(question, context):\n",
    "        return f\"question: {question} context: {context}\"\n",
    "    \n",
    "    input_text = [_to_input(q, c) for q, c in zip(batch['question'], batch['context'])]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        input_text,\n",
    "        max_length=16384,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=tokenized.input_ids.to(device),\n",
    "        attention_mask=tokenized.attention_mask.to(device),\n",
    "        max_length=512,\n",
    "    )\n",
    "    batch['predicted_answer'] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset.map(\n",
    "        generate_answer,\n",
    "        batched=True,\n",
    "        batch_size=4,\n",
    "        remove_columns=[\"question\", \"context\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "y_true = [a['text'][0] for a in results['answers']]\n",
    "y_pred = results['predicted_answer']\n",
    "precision, recall, f1 = score(y_true, y_pred, lang=\"en\", verbose=True)\n",
    "print(f\"Precision: {precision.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": results['id'],\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"precision\": precision.numpy(),\n",
    "        \"recall\": recall.numpy(),\n",
    "        \"f1\": f1.numpy()\n",
    "    }\n",
    ")\n",
    "\n",
    "df.to_parquet(\"data/long_t5_zero_shot.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/askem/data/longt5_zeroshot.parquet\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
