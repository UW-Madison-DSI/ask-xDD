{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid search\n",
    "\n",
    "To address the limitations of the original embedding search method, we propose a hybrid search method that combines the embedding search with `ElasticSearch` via `XDD Articles API v2`. \n",
    "\n",
    "## Hybrid API workflow\n",
    "\n",
    "1. Perform `ElasticSearch` on the query string and return top 100 results at article level\n",
    "2. Query vector store with:\n",
    "    - pre-filter among the top 100 results from elastic search\n",
    "    - (optional) any additional term-based pre-filtering\n",
    "    - embedding search on the query string\n",
    "3. Return relevant documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "- New API route: `/hybrid`\n",
    "- [swagger doc](http://cosmos0001.chtc.wisc.edu:4502/docs#/default/hybrid_get_docs_hybrid_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from askem._experimental.testset_ta1 import load_testset, gpt_eval\n",
    "from typing import Optional, List\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTSET = load_testset()\n",
    "RETRIEVER_URL = \"http://retriever:4502\"\n",
    "TESTSET.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can the new implementation helps on our worst performing segment?\n",
    "\n",
    "The query with key terms are among the worst performing items in v0 retriever, so we target these queries for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from askem.terms_extractor import MoreThanOneCapStrategy, get_blacklist\n",
    "\n",
    "df = TESTSET.query(\"is_keyword == 1\").copy()\n",
    "get_terms = MoreThanOneCapStrategy(\n",
    "    min_length=3, min_occurrence=1, top_k=3, blacklist=get_blacklist(\"covid\")\n",
    ")\n",
    "df[\"terms\"] = df[\"question\"].apply(get_terms.extract_terms)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We submit the same queries to the new API and compare the results with previous versions.\n",
    "\n",
    "1. XDD Articles V2\n",
    "2. Retriever V0\n",
    "3. Retriever Hybrid\n",
    "4. Retriever Hybrid + term-based pre-filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_xdd(query: str):\n",
    "    url = \"https://xdd.wisc.edu/api/v2/articles\"\n",
    "\n",
    "    params = {\n",
    "        \"term\": query,\n",
    "        \"dataset\": \"xdd-covid-19\",\n",
    "        # 'include_highlights': True,\n",
    "        \"include_score\": True,\n",
    "        # 'facets': True,\n",
    "        \"additional_fields\": \"title,abstract\",\n",
    "        \"match\": \"true\",\n",
    "        \"max\": 1,\n",
    "        # 'per_page': 20\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    return [r[\"abstract\"] for r in response.json()[\"success\"][\"data\"]]\n",
    "\n",
    "\n",
    "def query_retriever(body: dict, endpoint: Optional[str] = None) -> requests.Response:\n",
    "    \"\"\"Simulate Terarium query.\"\"\"\n",
    "\n",
    "    url = RETRIEVER_URL\n",
    "\n",
    "    if endpoint:\n",
    "        url = f\"{url}/{endpoint}\"\n",
    "\n",
    "    if \"top_k\" not in body:\n",
    "        body[\"top_k\"] = 1\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Api-Key\": os.getenv(\"RETRIEVER_APIKEY\"),\n",
    "    }\n",
    "    response = requests.post(url, json=body, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return [r[\"text\"] for r in response.json()]\n",
    "\n",
    "\n",
    "def compare(case: pd.Series, skip_term_based: bool = False) -> str:\n",
    "    \"\"\"Compare the results of different API versions.\"\"\"\n",
    "\n",
    "    output = f\"\"\"Question: {case.question}\n",
    "\n",
    "    XDD API (article level, showing abstract only):\n",
    "    {query_xdd(case.question)}\n",
    "\n",
    "    V0 retrieve API:\n",
    "    {query_retriever({\"question\": case.question})}\n",
    "\n",
    "    Hybrid API (No terms pre-filtering):\n",
    "    {query_retriever({\"question\": case.question}, \"hybrid\")}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if not skip_term_based:\n",
    "        output += f\"\"\"\n",
    "            Hybrid API (With terms pre-filtering):\n",
    "            {query_retriever({\"question\": case.question, \"paragraph_terms\": case.terms}, \"hybrid\")}\n",
    "            \"\"\"\n",
    "\n",
    "    return output.replace(\"    \", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = compare(df.iloc[0])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy quantification with GPT-4\n",
    "\n",
    "Comparing the performance is difficult without a ground truth. But we can bypass this problem with a lazy quantification method. We use GPT-4 prompt engineering to compare the performance of each API endpoint. It only serve as a rough quantification, but it is better than nothing. Manual examination is still required to confirm the results. \n",
    "\n",
    "```python\n",
    "\n",
    "# Prompt used\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a expert in epidemiology. Given the following evaluation results, select the best API for the given question. Tie is allowed. You organize your output like this: ['API1', 'API2', 'API3'] returning one or more best APIs. Return 'None' if you think none of the APIs are good.\",\n",
    "}\n",
    "\n",
    "user_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"Given this results: {result}, which API is the best?\",\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_chain(case: pd.Series, skip_term_based=False) -> dict:\n",
    "    \"\"\"Evaluate the results of different API versions with GPT.\"\"\"\n",
    "\n",
    "    raw = compare(case, skip_term_based=skip_term_based)\n",
    "    return {\n",
    "        \"question\": case.question,\n",
    "        \"raw_eval\": raw,\n",
    "        \"gpt_eval\": gpt_eval(raw, model=\"gpt-4\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all comparisons in the queries with `terms`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[r[\"question\"] for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for _, case in tqdm(df.iterrows()):\n",
    "    if case.question in [r[\"question\"] for r in results]:\n",
    "        continue\n",
    "    results.append(eval_chain(case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to json\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"results.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many votes are casted to each API?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_votes(results: list) -> dict:\n",
    "    # print cases\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"{i}: {result['question']} --- {result['gpt_eval']}\")\n",
    "\n",
    "    # flatten votes\n",
    "    votes = [r[\"gpt_eval\"] for r in results]\n",
    "    flat_votes = [item for sublist in votes for item in sublist if sublist != \"None\"]\n",
    "    stat = {v: flat_votes.count(v) for v in flat_votes}\n",
    "\n",
    "    # Sort votes by value\n",
    "    return {\n",
    "        k: v for k, v in sorted(stat.items(), key=lambda item: item[1], reverse=True)\n",
    "    }\n",
    "\n",
    "\n",
    "count_votes(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine tie situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in enumerate(results):\n",
    "    print(f\"{i}: {result['question']} --- {result['gpt_eval']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine(results: list, i: int) -> None:\n",
    "    print(\n",
    "        results[i][\"raw_eval\"],\n",
    "        \"\\n\",\n",
    "        f\"Votes: {results[i]['gpt_eval']}\",\n",
    "        \"\\n\",\n",
    "        \"=\" * 160,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding 1: Hybrid API perhaps works better than other reference APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine(results, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine(results, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding 2: Term-pre filtering can be too strict sometimes\n",
    "\n",
    "In some case, term-based pre-filtering is too stringent, Null result might be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine(results, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine(results, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine(results, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding 3: Occasionally all APIs work good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine(results, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding 4: Despite XDD comparison is not fair (only using abstract), it still win occasionally, but it is hard quantify further.\n",
    "\n",
    "It is difficult to tell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine(results, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding 5: We still have room for improvement, where none of the API works well (case: 3, 4, 13, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine(results, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine(results, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine(results, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- difficult question... probably should improve the question itself..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine(results, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can the hybrid-based search works in non-terms-based scenarios?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = TESTSET.query(\"is_keyword == 0\").copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_non_term = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "for i, case in tqdm(df.iterrows()):\n",
    "    if case.question in [r[\"question\"] for r in results_non_term]:\n",
    "        continue\n",
    "    results_non_term.append(eval_chain(case, skip_term_based=True))\n",
    "    if (i + 1) % 10 == 0:\n",
    "        sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results_non_term.json\", \"w\") as f:\n",
    "    json.dump(results_non_term, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_votes(results_non_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_all(results: list) -> None:\n",
    "    [examine(results, i) for i in range(len(results))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine_all(results_non_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
