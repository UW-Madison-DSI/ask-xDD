{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revamp data ingest pipeline\n",
    "\n",
    "New requirements:\n",
    "1. Have a resume mechanism to avoid duplication of text\n",
    "2. Have a append topic mechanism to modify existing paragraph topics.\n",
    "3. Don't break production demo.\n",
    "4. Directly ingest from ElasticSearch service without writing too much txt to disk.\n",
    "5. Make a cron job to do this automatically.\n",
    "\n",
    "Files to be ingested:\n",
    "\n",
    "- /hdd/iaross/askem/criticalmaas_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "1. Make a new Class: `Paragraph` to replace `Passage` class. `Passage` will be deprecated after the entire migration is done.\n",
    "1. Create canonical `id2topics` pickle file. Hopefully it is small enough to be loaded into memory.\n",
    "1. Use batch mechanism to ingest data from ElasticSearch service. e.g., 1000 paragraphs per batch.\n",
    "1. Upgrade frontend to use `Paragraph` class.\n",
    "1. Setup cron job to do this automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ln -s /hdd/iaross/askem/criticalmaas_text ./ingest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepartions\n",
    "\n",
    "Dump current id and topic to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import weaviate\n",
    "import hashlib\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = weaviate.auth.AuthApiKey(os.getenv(\"WEAVIATE_APIKEY\"))\n",
    "client = weaviate.Client(os.getenv(\"WEAVIATE_URL\"), auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check backup status\n",
    "client.backup.get_create_status(\n",
    "    backup_id=\"pre_duduplication\",\n",
    "    backend=\"filesystem\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_with_cursor(\n",
    "    client, class_name, class_properties, batch_size, cursor=None\n",
    "):\n",
    "    query = (\n",
    "        client.query.get(class_name, class_properties)\n",
    "        .with_additional([\"id\"])\n",
    "        .with_limit(batch_size)\n",
    "    )\n",
    "\n",
    "    if cursor is not None:\n",
    "        return query.with_after(cursor).do()\n",
    "    else:\n",
    "        return query.do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(text):\n",
    "    return hashlib.sha256(text.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of documents\n",
    "\n",
    "metadata = client.query.aggregate(\"passage\").with_meta_count().do()\n",
    "n = metadata[\"data\"][\"Aggregate\"][\"Passage\"][0][\"meta\"][\"count\"]\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump topic to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = None\n",
    "class_name = \"Passage\"\n",
    "id2topic = {}\n",
    "\n",
    "pbar = tqdm(total=n)\n",
    "while True:\n",
    "    # From the SOURCE instance, get the next group of objects\n",
    "    results = get_batch_with_cursor(\n",
    "        client,\n",
    "        class_name,\n",
    "        class_properties=[\"paper_id\", \"topic\"],\n",
    "        batch_size=1024,\n",
    "        cursor=cursor,\n",
    "    )\n",
    "\n",
    "    # If empty, we're finished\n",
    "    if len(results[\"data\"][\"Get\"][class_name]) == 0:\n",
    "        break\n",
    "\n",
    "    # A batch of objects\n",
    "    objects = results[\"data\"][\"Get\"][class_name]\n",
    "    for object in objects:\n",
    "        paper_id = object[\"paper_id\"]\n",
    "        topic = object[\"topic\"]\n",
    "\n",
    "        if paper_id not in id2topic:\n",
    "            id2topic[paper_id] = [topic]\n",
    "        elif topic not in id2topic[paper_id]:\n",
    "            id2topic[paper_id].append(topic)\n",
    "\n",
    "    # Update the cursor to the id of the last retrieved object\n",
    "    cursor = results[\"data\"][\"Get\"][class_name][-1][\"_additional\"][\"id\"]\n",
    "    pbar.update(len(objects))\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "# today date in YYMMDD format\n",
    "today = datetime.datetime.now().strftime(\"%y%m%d\")\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"topic_dump_{today}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id2topic, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduplicate with text hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = \"Passage\"\n",
    "existing_hash = set()\n",
    "batch_size = 32\n",
    "cursor = None\n",
    "deleted = 0\n",
    "pbar = tqdm(total=n)\n",
    "while True:\n",
    "    # From the SOURCE instance, get the next group of objects\n",
    "    results = get_batch_with_cursor(\n",
    "        client,\n",
    "        class_name,\n",
    "        class_properties=[\"paper_id\", \"text_content\"],\n",
    "        batch_size=batch_size,\n",
    "        cursor=cursor,\n",
    "    )\n",
    "\n",
    "    # If empty, we're finished\n",
    "    if len(results[\"data\"][\"Get\"][class_name]) == 0:\n",
    "        break\n",
    "\n",
    "    # A batch of objects\n",
    "    objects = results[\"data\"][\"Get\"][class_name]\n",
    "    for object in objects:\n",
    "        uuid = object[\"_additional\"][\"id\"]\n",
    "        paper_id = object[\"paper_id\"]\n",
    "        text = object[\"text_content\"]\n",
    "        hashed_text = get_hash(text)\n",
    "\n",
    "        if hashed_text not in existing_hash:\n",
    "            print(f\"Updating object: {uuid}\")\n",
    "            existing_hash.add(hashed_text)\n",
    "            client.data_object.update(\n",
    "                uuid=uuid,\n",
    "                class_name=class_name,\n",
    "                data_object={\n",
    "                    \"topic_list\": id2topic[paper_id],\n",
    "                    \"text_hash\": hashed_text,\n",
    "                },\n",
    "            )\n",
    "        else:\n",
    "            # Delete the duplicated object\n",
    "            print(f\"Deleting object: {uuid}\")\n",
    "            try:\n",
    "                client.data_object.delete(uuid, class_name)\n",
    "                deleted += 1\n",
    "            except weaviate.exceptions.UnexpectedStatusCodeException as e:\n",
    "                print(e)\n",
    "\n",
    "    # Update the cursor to the id of the last retrieved object\n",
    "    cursor = results[\"data\"][\"Get\"][class_name][-1][\"_additional\"][\"id\"]\n",
    "    pbar.update(len(objects))\n",
    "pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
