{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revamp data ingest pipeline\n",
    "\n",
    "New requirements:\n",
    "1. Have a resume mechanism to avoid duplication of text\n",
    "2. Have a append topic mechanism to modify existing paragraph topics.\n",
    "3. Don't break production demo.\n",
    "4. Directly ingest from ElasticSearch service without writing too much txt to disk.\n",
    "5. Make a cron job to do this automatically.\n",
    "\n",
    "Files to be ingested:\n",
    "\n",
    "- /hdd/iaross/askem/criticalmaas_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "1. Make a new Class: `Paragraph` to replace `Passage` class. `Passage` will be deprecated after the entire migration is done.\n",
    "1. Create canonical `id2topics` pickle file. Hopefully it is small enough to be loaded into memory.\n",
    "1. Use batch mechanism to ingest data from ElasticSearch service. e.g., 1000 paragraphs per batch.\n",
    "1. Upgrade frontend to use `Paragraph` class.\n",
    "1. Setup cron job to do this automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import weaviate\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Temporary fix to get the path right\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/hdd/clo36/repo/ask-xDD/askem/retriever\")\n",
    "\n",
    "from askem.retriever.base import get_schema\n",
    "from askem.utils import get_ingested_ids, get_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create `Paragraph` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd/clo36/repo/ask-xDD/venv/lib/python3.10/site-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.25.3. The latest version is 4.4.0.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "weaviate_client = weaviate.Client(\n",
    "    url=os.getenv(\"WEAVIATE_URL\"),\n",
    "    auth_client_secret=weaviate.AuthApiKey(api_key=os.getenv(\"WEAVIATE_APIKEY\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client.query.aggregate(\"Passage\").with_meta_count().do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_schema = get_schema(\"Paragraph\")\n",
    "paragraph_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client.schema.create_class(paragraph_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client.query.aggregate(\"Paragraph\").with_meta_count().do()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create id2topics pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert(d: dict[str : list[str]]) -> dict[str : list[str]]:\n",
    "    \"\"\"Invert a dictionary.\"\"\"\n",
    "    inverted = {}\n",
    "    for topic, ids in d.items():\n",
    "        for id in ids:\n",
    "            if id not in inverted:\n",
    "                inverted[id] = [topic]\n",
    "            elif topic not in inverted[id]:\n",
    "                inverted[id].append(topic)\n",
    "    return inverted\n",
    "\n",
    "\n",
    "class DocumentTopicFactory:\n",
    "    \"\"\"A factory class to create document-topic objects.\"\"\"\n",
    "\n",
    "    def __init__(self, set_names: list[str]) -> None:\n",
    "        self.set_names = set_names\n",
    "\n",
    "        self.id2topics: dict[str : list[str]] = {}\n",
    "        self.topic2ids: dict[str : list[str]] = {}\n",
    "\n",
    "    def run(self) -> dict[str : list[str]]:\n",
    "        \"\"\"Run the factory.\"\"\"\n",
    "        for set_name in self.set_names:\n",
    "            print(f\"Getting ids for {set_name}\")\n",
    "            self.topic2ids[set_name] = self.get_ids(set_name)\n",
    "            print(f\"Found {len(self.topic2ids[set_name])} ids for {set_name}\")\n",
    "\n",
    "        self.id2topics = invert(self.topic2ids)\n",
    "\n",
    "        # Write to file\n",
    "        with open(\"tmp/id2topics.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.id2topics, f)\n",
    "\n",
    "        return self.id2topics\n",
    "\n",
    "    def get_ids(self, topic: str) -> list[str]:\n",
    "        \"\"\"Get all ids for a topic.\"\"\"\n",
    "\n",
    "        next_page = f\"https://xdd.wisc.edu/api/articles?set={topic}&full_results=true&fields=_gddid\"\n",
    "        progress_bar = tqdm()\n",
    "        ids = []\n",
    "        while next_page:\n",
    "            response = requests.get(next_page)\n",
    "            data = response.json()\n",
    "            ids.extend(self._parse_response(data))\n",
    "            next_page = data[\"success\"][\"next_page\"]\n",
    "            progress_bar.update(1)\n",
    "        return ids\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"\\n\".join(\n",
    "            [f\"{topic}: n={len(ids)}\" for topic, ids in self.topic2ids.items()]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_response(data: dict) -> list[str]:\n",
    "        \"\"\"Get all ids from a xDD json response.\"\"\"\n",
    "\n",
    "        if \"success\" not in data:\n",
    "            raise ValueError(\"Not a valid xDD response.\")\n",
    "\n",
    "        docs = data[\"success\"][\"data\"]\n",
    "        return [doc[\"_gddid\"] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_names = [\n",
    "    \"climate-change-modeling\",\n",
    "    \"criticalmaas\",\n",
    "    \"dolomites\",\n",
    "    \"geoarchive\",\n",
    "    \"xdd-covid-19\",\n",
    "]\n",
    "doc_topic_factory = DocumentTopicFactory(set_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2topics = doc_topic_factory.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Ingest into `Paragraph` class directly from ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeaviateIngester:\n",
    "    def __init__(\n",
    "        self, client: weaviate.Client, id2topics_pkl: Path, ingested_pkl: Path\n",
    "    ) -> None:\n",
    "        self.client = client\n",
    "\n",
    "        with open(id2topics_pkl, \"rb\") as f:\n",
    "            self.id2topics = pickle.load(f)\n",
    "\n",
    "        with open(ingested_pkl, \"rb\") as f:\n",
    "            self.ingested = pickle.load(f)\n",
    "\n",
    "\n",
    "ingester = WeaviateIngester(\n",
    "    client=weaviate_client,\n",
    "    id2topics_pkl=\"tmp/id2topics.pkl\",\n",
    "    ingested_pkl=\"tmp/ingested.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingested_ids = get_ingested_ids(weaviate_client, class_name=\"Paragraph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingested_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tmp/id2topics.pkl\", \"rb\") as f:\n",
    "    id2topics = pickle.load(f)\n",
    "id2topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "batch_size = 10\n",
    "batch_ids = sorted(id2topics.keys())[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ingest_tmp_folder = Path(\"tmp/ingest\")\n",
    "\n",
    "\n",
    "def write_batch_to_file(batch_ids: list[str], folder: Path) -> None:\n",
    "    \"\"\"Write a batch of ids to a tmp file.\"\"\"\n",
    "\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for docid in batch_ids:\n",
    "        text = get_text(docid)\n",
    "        with open(f\"{folder}/{docid}.txt\", \"w\") as f:\n",
    "            f.write(text)\n",
    "\n",
    "\n",
    "write_batch_to_file(batch_ids, ingest_tmp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from askem.preprocessing import HaystackPreprocessor\n",
    "\n",
    "preprocessor = HaystackPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = ingest_tmp_folder.glob(\"**/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "doc_type = \"paragraph\"\n",
    "class_name = \"Paragraph\"\n",
    "weaviate_client.batch.configure(batch_size=5, dynamic=True)\n",
    "\n",
    "with weaviate_client.batch as batch:\n",
    "    for input_file in input_files:\n",
    "        docid = input_file.stem\n",
    "        topics = id2topics[docid]\n",
    "        docs = preprocessor.run(input_file=input_file, topics=topics, doc_type=doc_type)\n",
    "\n",
    "        # paragraph level loop (each paragraph)\n",
    "        for doc in docs:\n",
    "            batch.add_data_object(data_object=doc, class_name=class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client.query.aggregate(\"Paragraph\").with_meta_count().do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_names = [\n",
    "    \"climate-change-modeling\",\n",
    "    \"criticalmaas\",\n",
    "    \"dolomites\",\n",
    "    \"geoarchive\",\n",
    "    \"xdd-covid-19\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client.query.get(\n",
    "    \"Paragraph\", [\"paper_id\", \"topic_list\", \"hashed_text\"]\n",
    ").with_limit(5).do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weaviate_client.query.get(\"Paragraph\", [\"paper_id\", \"topic_list\", \"hashed_text\"]).with_where(\n",
    "#     {\n",
    "#         \"path\": \"topic_list\",\n",
    "#         \"operator\": \"ContainsAny\",\n",
    "#         \"valueText\": [\"geoarchive\"],\n",
    "#     }\n",
    "# ).with_limit(5).do()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepartions\n",
    "\n",
    "Dump current id and topic to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import weaviate\n",
    "import hashlib\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = weaviate.auth.AuthApiKey(os.getenv(\"WEAVIATE_APIKEY\"))\n",
    "client = weaviate.Client(os.getenv(\"WEAVIATE_URL\"), auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check backup status\n",
    "client.backup.get_create_status(\n",
    "    backup_id=\"pre_duduplication\",\n",
    "    backend=\"filesystem\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_with_cursor(\n",
    "    client, class_name, class_properties, batch_size, cursor=None\n",
    "):\n",
    "    query = (\n",
    "        client.query.get(class_name, class_properties)\n",
    "        .with_additional([\"id\"])\n",
    "        .with_limit(batch_size)\n",
    "    )\n",
    "\n",
    "    if cursor is not None:\n",
    "        return query.with_after(cursor).do()\n",
    "    else:\n",
    "        return query.do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(text):\n",
    "    return hashlib.sha256(text.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of documents\n",
    "\n",
    "metadata = client.query.aggregate(\"passage\").with_meta_count().do()\n",
    "n = metadata[\"data\"][\"Aggregate\"][\"Passage\"][0][\"meta\"][\"count\"]\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump topic to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = None\n",
    "class_name = \"Passage\"\n",
    "id2topic = {}\n",
    "\n",
    "pbar = tqdm(total=n)\n",
    "while True:\n",
    "    # From the SOURCE instance, get the next group of objects\n",
    "    results = get_batch_with_cursor(\n",
    "        client,\n",
    "        class_name,\n",
    "        class_properties=[\"paper_id\", \"topic\"],\n",
    "        batch_size=1024,\n",
    "        cursor=cursor,\n",
    "    )\n",
    "\n",
    "    # If empty, we're finished\n",
    "    if len(results[\"data\"][\"Get\"][class_name]) == 0:\n",
    "        break\n",
    "\n",
    "    # A batch of objects\n",
    "    objects = results[\"data\"][\"Get\"][class_name]\n",
    "    for object in objects:\n",
    "        paper_id = object[\"paper_id\"]\n",
    "        topic = object[\"topic\"]\n",
    "\n",
    "        if paper_id not in id2topic:\n",
    "            id2topic[paper_id] = [topic]\n",
    "        elif topic not in id2topic[paper_id]:\n",
    "            id2topic[paper_id].append(topic)\n",
    "\n",
    "    # Update the cursor to the id of the last retrieved object\n",
    "    cursor = results[\"data\"][\"Get\"][class_name][-1][\"_additional\"][\"id\"]\n",
    "    pbar.update(len(objects))\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "# today date in YYMMDD format\n",
    "today = datetime.datetime.now().strftime(\"%y%m%d\")\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"topic_dump_{today}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id2topic, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduplicate with text hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = \"Passage\"\n",
    "existing_hash = set()\n",
    "batch_size = 32\n",
    "cursor = None\n",
    "deleted = 0\n",
    "pbar = tqdm(total=n)\n",
    "while True:\n",
    "    # From the SOURCE instance, get the next group of objects\n",
    "    results = get_batch_with_cursor(\n",
    "        client,\n",
    "        class_name,\n",
    "        class_properties=[\"paper_id\", \"text_content\"],\n",
    "        batch_size=batch_size,\n",
    "        cursor=cursor,\n",
    "    )\n",
    "\n",
    "    # If empty, we're finished\n",
    "    if len(results[\"data\"][\"Get\"][class_name]) == 0:\n",
    "        break\n",
    "\n",
    "    # A batch of objects\n",
    "    objects = results[\"data\"][\"Get\"][class_name]\n",
    "    for object in objects:\n",
    "        uuid = object[\"_additional\"][\"id\"]\n",
    "        paper_id = object[\"paper_id\"]\n",
    "        text = object[\"text_content\"]\n",
    "        hashed_text = get_hash(text)\n",
    "\n",
    "        if hashed_text not in existing_hash:\n",
    "            print(f\"Updating object: {uuid}\")\n",
    "            existing_hash.add(hashed_text)\n",
    "            client.data_object.update(\n",
    "                uuid=uuid,\n",
    "                class_name=class_name,\n",
    "                data_object={\n",
    "                    \"topic_list\": id2topic[paper_id],\n",
    "                    \"text_hash\": hashed_text,\n",
    "                },\n",
    "            )\n",
    "        else:\n",
    "            # Delete the duplicated object\n",
    "            print(f\"Deleting object: {uuid}\")\n",
    "            try:\n",
    "                client.data_object.delete(uuid, class_name)\n",
    "                deleted += 1\n",
    "            except weaviate.exceptions.UnexpectedStatusCodeException as e:\n",
    "                print(e)\n",
    "\n",
    "    # Update the cursor to the id of the last retrieved object\n",
    "    cursor = results[\"data\"][\"Get\"][class_name][-1][\"_additional\"][\"id\"]\n",
    "    pbar.update(len(objects))\n",
    "pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
