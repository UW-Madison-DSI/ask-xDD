{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Testset and Just look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import gspread\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import List, Optional\n",
    "from askem.preprocessing import get_all_cap_words\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_testset() -> pd.DataFrame:\n",
    "    \"\"\"Load testset from Google Sheet.\"\"\"\n",
    "\n",
    "    GCP_SECRET_FILE_PATH = os.getenv(\"GCP_SECRET_FILE_PATH\")\n",
    "\n",
    "    gc = gspread.service_account(filename=GCP_SECRET_FILE_PATH)\n",
    "    sheet = gc.open(\"ASKEM-TA1-testset\").worksheet(\"questions\")\n",
    "\n",
    "    records = sheet.get_values()\n",
    "    labels = records[0]\n",
    "    data = records[1:]\n",
    "\n",
    "    new_labels = [label.lower().replace(\" \", \"_\") for label in labels]\n",
    "    df = pd.DataFrame.from_records(data, columns=new_labels)\n",
    "    return df[[\"source\", \"target_type\", \"is_keyword\", \"question\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"terms\"] = df[\"question\"].apply(get_all_cap_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"terms\", \"is_keyword\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we should not include some really common terms like: `COVID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLACKLIST_COMMON_TERMS = [\"COVID19\", \"COVID\"]\n",
    "\n",
    "\n",
    "def get_better_terms(text: str, blacklist: Optional[List[str]] = None) -> List[str]:\n",
    "    \"\"\"Get better terms from text.\"\"\"\n",
    "    terms = get_all_cap_words(text)\n",
    "    if not terms:\n",
    "        return None\n",
    "\n",
    "    if blacklist is None:\n",
    "        blacklist = BLACKLIST_COMMON_TERMS\n",
    "\n",
    "    better_terms = [term for term in terms if term not in blacklist]\n",
    "    if not better_terms:\n",
    "        return None\n",
    "    return better_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"terms\"] = df[\"question\"].apply(get_better_terms)\n",
    "df[[\"terms\", \"is_keyword\", \"question\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note. `COmplexVID` is not considered as a term. TODO: May need to address this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ta1(\n",
    "    question: str, article_terms: List[str] = None, paragraph_terms: List[str] = None\n",
    ") -> List[dict]:\n",
    "    \"\"\"Evaluate a question using the retriever API.\"\"\"\n",
    "\n",
    "    URL = os.getenv(\"RETRIEVER_URL\")\n",
    "    APIKEY = os.getenv(\"RETRIEVER_APIKEY\")\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Api-Key\": APIKEY}\n",
    "    json = {\"question\": question, \"top_k\": 10, \"doc_type\": \"paragraph\"}\n",
    "\n",
    "    if article_terms:\n",
    "        json[\"article_terms\"] = article_terms\n",
    "\n",
    "    if paragraph_terms:\n",
    "        json[\"paragraph_terms\"] = paragraph_terms\n",
    "\n",
    "    response = requests.post(URL, headers=headers, json=json)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all questions with terms\n",
    "\n",
    "df_with_terms = df.query(\"terms.notnull()\")\n",
    "\n",
    "results = []\n",
    "for row in df_with_terms.itertuples():\n",
    "    results.append(\n",
    "        {\n",
    "            \"question\": row.question,\n",
    "            \"terms\": row.terms,\n",
    "            \"results_with_article_level_filter\": eval_ta1(\n",
    "                row.question, article_terms=row.terms\n",
    "            ),\n",
    "            \"results_with_paragraph_level_filter\": eval_ta1(\n",
    "                row.question, paragraph_terms=row.terms\n",
    "            ),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
