{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search\n",
    "\n",
    "### Append required data to vector store\n",
    "1. Preprocess Raw Text: Extract paragraphs from the raw text.\n",
    "1. Identify Words: Use DPR tokenizer to get tokenized words.\n",
    "1. Parse text naively to get all words (remove punctuation and diacritics, then split).\n",
    "1. Find Untokenized Words:\n",
    "    - Untokenized words = all words - tokenized words.\n",
    "    - Add the top 3 most frequent untokenized words to each paragraph.\n",
    "    - Add the top 3 most frequent article-level untokenized words to each paragraph.\n",
    "1. Identify Capitalized Terms:\n",
    "    - Find capitalized terms with 3 or more characters.\n",
    "    - Add the top 3 most frequent capitalized terms to each paragraph.\n",
    "    - Add the top 3 most frequent article-level capitalized terms to each paragraph.\n",
    "\n",
    "### Query Workflow:\n",
    "\n",
    "1. First filter results using untokenized words or capitalized terms.\n",
    "2. Then proceed with dot-product embedding search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from askem.preprocessing import HaystackPreprocessor\n",
    "from transformers import DPRContextEncoderTokenizer\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "import hashlib\n",
    "import unicodedata\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DOCS = list(Path(\"data/covid1000\").glob(\"*.txt\"))[:100]\n",
    "OUTPUT_DIR = Path(\"data/hybrid_retrieval/experiment_4\")\n",
    "\n",
    "preprocessor = HaystackPreprocessor()\n",
    "tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def strip_punctuation(text: str) -> str:\n",
    "    return \"\".join([c for c in text if c.isalnum() or c.isspace()])\n",
    "\n",
    "def remove_diacritics(text: str) -> str:\n",
    "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
    "    return ''.join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "def get_non_tokenized_words(text, tokenizer, min_length = 3, top_k: int = 3) -> list:\n",
    "    \"\"\"Get words that are not tokenized by a tokenizer.\"\"\"\n",
    "\n",
    "    # Preprocess text\n",
    "    text = strip_punctuation(text).lower()\n",
    "    text = remove_diacritics(text)\n",
    "\n",
    "    all_words = set(text.split())\n",
    "\n",
    "    tokenized = tokenizer(text)[\"input_ids\"]\n",
    "    tokenized_words = set(tokenizer.decode(tokenized).split())\n",
    "\n",
    "    non_tokenized_words = all_words - tokenized_words\n",
    "    non_tokenized_words = [word for word in non_tokenized_words if len(word) >= min_length]\n",
    "\n",
    "    if not non_tokenized_words:\n",
    "        return None\n",
    "    \n",
    "    # Count the number of non-tokenized words\n",
    "    counts = {word: text.count(word) for word in non_tokenized_words}\n",
    "    \n",
    "    return sorted(counts, key=counts.get, reverse=True)[:top_k]\n",
    "\n",
    "\n",
    "def get_all_cap_words(text: str, min_length: int = 3, top_k: int = 3) -> list:\n",
    "    \"\"\"Get capitalized words in a text, sorted by number of occurrence.\"\"\"\n",
    "\n",
    "    text = strip_punctuation(text)\n",
    "    text = remove_diacritics(text)\n",
    "    \n",
    "    words = text.split()\n",
    "    all_cap_words = [word for word in words if word.isupper() and len(word) >= min_length]\n",
    "\n",
    "    if not all_cap_words:\n",
    "        return None\n",
    "    \n",
    "    # Count the number of all caps words\n",
    "    counts = {word: text.count(word) for word in all_cap_words}\n",
    "\n",
    "    # Return top-k most frequent all caps words\n",
    "    return sorted(counts, key=counts.get, reverse=True)[:top_k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many fields are too many?\n",
    "\n",
    "## To-Dos\n",
    "\n",
    "1. What is the distribution of n CAP and n NON-TOKENIZED looks like?\n",
    "2. Same as 1, but at article level.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paragraph(BaseModel):\n",
    "\n",
    "    id: Optional[str] = None\n",
    "    paper_id: str\n",
    "    text: str\n",
    "    non_tokenized_words: Optional[list] = None\n",
    "    all_cap_words: Optional[list] = None\n",
    "\n",
    "    # These are from parent article\n",
    "    article_non_tokenized_words: Optional[list] = None\n",
    "    article_all_cap_words: Optional[list] = None\n",
    "\n",
    "    def __init__(self, **data) -> None:\n",
    "        super().__init__(**data)\n",
    "        self.id = hashlib.md5(self.text.encode()).hexdigest()\n",
    "\n",
    "    def save(self, path: Optional[Path] = None) -> None:\n",
    "        if not path:\n",
    "            path = OUTPUT_DIR / f\"{self.id}.json\"\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(self.json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process all test documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPITALIZED_TERMS_COUNT = dict()\n",
    "NON_TOKENIZED_TERMS_COUNT = dict()\n",
    "\n",
    "def update_count(d: dict, words: Optional[List[str]]) -> None:\n",
    "    if not words:\n",
    "        return None\n",
    "    \n",
    "    for word in words:\n",
    "        if word in d:\n",
    "            d[word] += 1\n",
    "        else:\n",
    "            d[word] = 1\n",
    "\n",
    "\n",
    "def get_top_k(d: dict, k: int = 10, min_n: int = 3) -> dict:\n",
    "    \"\"\"Get top-k most frequent words in a dictionary.\"\"\"\n",
    "\n",
    "    d = {k: v for k, v in d.items() if v >= min_n}\n",
    "    return sorted(d, key=d.get, reverse=True)[:k]\n",
    "\n",
    "for i, article in enumerate(TEST_DOCS):\n",
    "    print(f\"Processing {i}/100: {article}...\")\n",
    "\n",
    "    paragraphs = preprocessor.run(input_file=article, topic=\"covid\", doc_type=\"paragraph\")\n",
    "\n",
    "    # Keep track of article-level information\n",
    "    this_outputs = []\n",
    "    this_article_non_tokenized_words_count = {}\n",
    "    this_article_all_cap_words_count = {}\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        text = paragraph[\"text_content\"]\n",
    "        non_tokenized_words = get_non_tokenized_words(text, tokenizer, top_k=3)\n",
    "        # print(f\"{non_tokenized_words=}\")\n",
    "        all_cap_words = get_all_cap_words(text, top_k=3)\n",
    "        # print(f\"{all_cap_words=}\")\n",
    "\n",
    "        this_outputs.append(\n",
    "            Paragraph(\n",
    "                paper_id = paragraph[\"paper_id\"],\n",
    "                text = text,\n",
    "                non_tokenized_words = non_tokenized_words,\n",
    "                all_cap_words = all_cap_words,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if non_tokenized_words:\n",
    "            update_count(this_article_non_tokenized_words_count, non_tokenized_words)\n",
    "            update_count(NON_TOKENIZED_TERMS_COUNT, non_tokenized_words)\n",
    "\n",
    "        if all_cap_words:\n",
    "            update_count(this_article_all_cap_words_count, all_cap_words)\n",
    "            update_count(CAPITALIZED_TERMS_COUNT, all_cap_words)\n",
    "\n",
    "    this_article_non_tokenized_words = get_top_k(this_article_non_tokenized_words_count, k=10, min_n=3)\n",
    "    print(this_article_non_tokenized_words)\n",
    "\n",
    "    this_article_all_cap_words = get_top_k(this_article_all_cap_words_count, k=10, min_n=3)\n",
    "    print(this_article_all_cap_words)\n",
    "\n",
    "    # Append article-level information to each paragraph\n",
    "    for output in this_outputs:\n",
    "        output.article_non_tokenized_words = this_article_non_tokenized_words\n",
    "        output.article_all_cap_words = this_article_all_cap_words\n",
    "        output.save()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 13m 30s for 1000 docs locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(CAPITALIZED_TERMS_COUNT))\n",
    "print(len(NON_TOKENIZED_TERMS_COUNT))\n",
    "\n",
    "with open(\"capitalized.json\", \"w\") as f:\n",
    "    f.write(json.dumps(CAPITALIZED_TERMS_COUNT, indent=4))\n",
    "\n",
    "with open(\"non_tokenized.json\", \"w\") as f:\n",
    "    f.write(json.dumps(NON_TOKENIZED_TERMS_COUNT, indent=4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 100 NON_TOKENIZED_TERMS (in 1000 test docs)\n",
    "top_100_nt_terms = sorted(NON_TOKENIZED_TERMS_COUNT, key=NON_TOKENIZED_TERMS_COUNT.get, reverse=True)[:100]\n",
    "print(top_100_nt_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encoding issue > capturing unique search term\n",
    "- the tokenizer have very high coverage\n",
    "- maybe not a good option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 100\n",
    "print(sorted(CAPITALIZED_TERMS_COUNT, key=CAPITALIZED_TERMS_COUNT.get, reverse=True)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom 100\n",
    "print(sorted(CAPITALIZED_TERMS_COUNT, key=CAPITALIZED_TERMS_COUNT.get, reverse=False)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CAPTERMS is somewhat closer to ASKEM use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many CAP terms are there at the paragraph level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "JSON_FILES = Path(\"data/hybrid_retrieval/experiment_4\").glob(\"*.json\")\n",
    "\n",
    "n_terms_paragraph = []\n",
    "n_terms_article = []\n",
    "for file in tqdm(list(JSON_FILES)):\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    n_para = data[\"all_cap_words\"]\n",
    "    if not n_para:\n",
    "        n_para = 0\n",
    "    else:\n",
    "        n_para = len(data[\"all_cap_words\"])\n",
    "\n",
    "    n_article = data[\"article_all_cap_words\"]\n",
    "    if not n_article: \n",
    "        n_article = 0\n",
    "    else:\n",
    "        n_article = len(data[\"article_all_cap_words\"])\n",
    "\n",
    "\n",
    "    n_terms_paragraph.append(n_para)\n",
    "    n_terms_article.append(n_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(n_terms_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, each paragraph has 1.5 terms. We can probably use 3 terms (fields) per paragraph in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(n_terms_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, each article has 27 terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"n_terms_paragraph\": n_terms_paragraph, \"n_terms_article\": n_terms_article})\n",
    "df['n_terms_article'].plot(kind=\"hist\", title=\"Number of capitalized terms in an article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal:\n",
    "Select a specific number of terms at the article level to append to each paragraph for search optimization.\n",
    "\n",
    "Considerations:\n",
    "\n",
    "    Too Many Terms:\n",
    "        Adding a lot of terms might slow down the search, although this needs to be verified through benchmarking.\n",
    "        From a precision perspective, more terms are generally better.\n",
    "        Possible Solution: Include terms that occur at least 3 times at the article level.\n",
    "\n",
    "    Too Few Terms:\n",
    "        A smaller number of terms could compromise search accuracy.\n",
    "\n",
    "Initial Plan:\n",
    "\n",
    "    Start by implementing 50 terms per paragraph as a baseline.\n",
    "    Conduct benchmark tests to validate the effectiveness and speed of the search with this setting."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
