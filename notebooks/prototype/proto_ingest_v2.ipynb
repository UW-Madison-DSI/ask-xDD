{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revamp data ingest pipeline\n",
    "\n",
    "New requirements:\n",
    "1. Have a resume mechanism to avoid duplication of text\n",
    "2. Have a append topic mechanism to modify existing paragraph topics.\n",
    "3. Don't break production demo.\n",
    "\n",
    "Files to be ingested:\n",
    "\n",
    "- /hdd/iaross/askem/criticalmaas_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ln -s /hdd/iaross/askem/criticalmaas_text ./ingest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepartions\n",
    "\n",
    "1. Dump current id and topic to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import weaviate\n",
    "import hashlib\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = weaviate.auth.AuthApiKey(os.getenv(\"WEAVIATE_APIKEY\"))\n",
    "client = weaviate.Client(os.getenv(\"WEAVIATE_URL\"), auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check backup status\n",
    "client.backup.get_create_status(\n",
    "    backup_id=\"pre_duduplication\",\n",
    "    backend=\"filesystem\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_with_cursor(\n",
    "    client, class_name, class_properties, batch_size, cursor=None\n",
    "):\n",
    "    query = (\n",
    "        client.query.get(class_name, class_properties)\n",
    "        .with_additional([\"id\"])\n",
    "        .with_limit(batch_size)\n",
    "    )\n",
    "\n",
    "    if cursor is not None:\n",
    "        return query.with_after(cursor).do()\n",
    "    else:\n",
    "        return query.do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(text):\n",
    "    return hashlib.sha256(text.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21996410"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of documents\n",
    "\n",
    "metadata = client.query.aggregate(\"passage\").with_meta_count().do()\n",
    "n = metadata[\"data\"][\"Aggregate\"][\"Passage\"][0][\"meta\"][\"count\"]\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump topic to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21996410/21996410 [32:09<00:00, 11400.17it/s]\n"
     ]
    }
   ],
   "source": [
    "cursor = None\n",
    "class_name = \"Passage\"\n",
    "id2topic = {}\n",
    "\n",
    "pbar = tqdm(total=n)\n",
    "while True:\n",
    "    # From the SOURCE instance, get the next group of objects\n",
    "    results = get_batch_with_cursor(\n",
    "        client,\n",
    "        class_name,\n",
    "        class_properties=[\"paper_id\", \"topic\"],\n",
    "        batch_size=1024,\n",
    "        cursor=cursor,\n",
    "    )\n",
    "\n",
    "    # If empty, we're finished\n",
    "    if len(results[\"data\"][\"Get\"][class_name]) == 0:\n",
    "        break\n",
    "\n",
    "    # A batch of objects\n",
    "    objects = results[\"data\"][\"Get\"][class_name]\n",
    "    for object in objects:\n",
    "        paper_id = object[\"paper_id\"]\n",
    "        topic = object[\"topic\"]\n",
    "\n",
    "        if paper_id not in id2topic:\n",
    "            id2topic[paper_id] = [topic]\n",
    "        elif topic not in id2topic[paper_id]:\n",
    "            id2topic[paper_id].append(topic)\n",
    "\n",
    "    # Update the cursor to the id of the last retrieved object\n",
    "    cursor = results[\"data\"][\"Get\"][class_name][-1][\"_additional\"][\"id\"]\n",
    "    pbar.update(len(objects))\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"topic_dump_240129.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id2topic, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduplicate with text hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = None\n",
    "class_name = \"Passage\"\n",
    "existing_hash = set()\n",
    "deleted = 0\n",
    "pbar = tqdm(total=n)\n",
    "while True:\n",
    "    # From the SOURCE instance, get the next group of objects\n",
    "    results = get_batch_with_cursor(\n",
    "        client,\n",
    "        class_name,\n",
    "        class_properties=[\"paper_id\", \"text_content\"],\n",
    "        batch_size=64,\n",
    "        cursor=cursor,\n",
    "    )\n",
    "\n",
    "    # If empty, we're finished\n",
    "    if len(results[\"data\"][\"Get\"][class_name]) == 0:\n",
    "        break\n",
    "\n",
    "    # A batch of objects\n",
    "    objects = results[\"data\"][\"Get\"][class_name]\n",
    "    for object in objects:\n",
    "        uuid = object[\"_additional\"][\"id\"]\n",
    "        paper_id = object[\"paper_id\"]\n",
    "        text = object[\"text_content\"]\n",
    "        hashed_text = get_hash(text)\n",
    "\n",
    "        if hashed_text not in existing_hash:\n",
    "            existing_hash.add(hashed_text)\n",
    "        else:\n",
    "            # Delete the duplicated object\n",
    "            try:\n",
    "                client.data_object.delete(uuid, class_name)\n",
    "                deleted += 1\n",
    "            except weaviate.exceptions.UnexpectedStatusCodeException as e:\n",
    "                print(e)\n",
    "\n",
    "    # Update the cursor to the id of the last retrieved object\n",
    "    cursor = results[\"data\"][\"Get\"][class_name][-1][\"_additional\"][\"id\"]\n",
    "    pbar.update(len(objects))\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backend': 'filesystem',\n",
       " 'id': 'pre_duduplication',\n",
       " 'path': '/tmp/backups/pre_duduplication',\n",
       " 'status': 'STARTED'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
